{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "##### For a better and more in depth explanation of Resampling methods, refer to the free online book \"An Introduction to Statistical Learning with Applications in R by James et. al. [http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\n",
    "\n",
    "Another very good books is The Elements of Statistical Learning by Hastie et al 2017. \n",
    "[https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf](https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf) \n",
    "\n",
    "Most of you are familiar with the terms *training* and *testing* sets which refer to splitting the data into two subsets that will be used to \"develop\" the model - the learning step - and a to test the model -the evaluation step. \n",
    "\n",
    "For example: if we want to understand how much variation we can expect from a linear regression, using resampling methods we can draw multiple random samples from the training data, fit a linear regression to each sample, and measure how much variation there is across subsets.\n",
    "\n",
    "We have already seen few aspects of resampling methods when we used boostrapping to obtain confidence intervals around a mean. Boostrapping is an efficient and powerful method to increase the level of accuracy of a parameter estimate.\n",
    "\n",
    "Another resampling technique that will be the focus for this lecture is Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "Cross-validation can be used to evaluate the performance of a statistical test, and to estimate a measurement of error from this statistical analysis more accurately.\n",
    "\n",
    "The basic concept as mentioned before is to partition the data into subsets that can be used to measure the error differences across subsets and to evaluate the performance using a hold out set or test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, one of the most frequently used error estimators for regression models is the MSE(mean squared error), which is a measurement of quality of the fit for a particular model.\n",
    "\n",
    "The idea is to measure how close is the predicted response for each observation to the true response value for that observation.\n",
    "\n",
    "$$ MSE = \\frac {1}{n} \\sum _{i = 1}^{n} (y_i - \\hat {f}(x_i))^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\hat{f}_i$ is the prediction of $\\hat{f}$ on each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = read.csv(\"diabetes.csv\", header = T, stringsAsFactors = F)\n",
    "diabetes = diabetes[!diabetes$Glucose==0,]\n",
    "diabetes = diabetes[!diabetes$Insulin==0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot data and regression line\n",
    "\n",
    "dia_lm = lm(diabetes$Insulin ~ diabetes$Glucose)\n",
    "plot(diabetes$Glucose, diabetes$Insulin)\n",
    "lines(diabetes$Glucose, dia_lm$fitted.values)\n",
    "\n",
    "cor(diabetes$Glucose, diabetes$Insulin)\n",
    "\n",
    "sm = summary(dia_lm)\n",
    "summary(dia_lm)\n",
    "##Find mse\n",
    "anova(Muscle)\n",
    "\n",
    "92.14^2 ##From the Residual standard error in the summary of the model\n",
    "\n",
    "##AIC is another heaviliy used meausrement of the fit of the data, the lower the number\n",
    "##the better the fit.\n",
    "AIC(dia_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse <- function(error)\n",
    "{\n",
    "  sqrt(mean(error^2))\n",
    "}\n",
    "rmse(dia_lm$residuals)\n",
    "\n",
    "RSS <- c(crossprod(dia_lm$residuals))\n",
    "MSE <- RSS / length(dia_lm$residuals)\n",
    "RMSE <- sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MSE is the overall MSE for the model, however, we want to evaluate how well the method works on an untested set (partition) not used to train the model.\n",
    "\n",
    "One option would also be to calculate the MSE from the model used for training and ideally this values is very small giving an indication of how good the model is for that partition. However, we don't really care to know how well the model predict what we already know, or to calculate a training MSE. What we want to do is to evaluate the model to a subset that has not been used to evaluate the model.\n",
    "\n",
    "We want to chose a method that gives the lowest MSE, as opposed to the lowest training MSE, if we have a large number of test observations we can calculate the average MSE and select a model with the lowest test MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is the model in predicting the data\n",
    "\n",
    "By using the whole dataset to run the model we have no way to test how good the model is in predicting new data, for this reason we will split the dataset into training and testing (80/20) sample.\n",
    "\n",
    "Once we are able to use the model to predict values from the testing samples we can evaluate accuracy and error rates of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Test data -\n",
    "set.seed(100)  # setting seed to reproduce results of random sampling\n",
    "trainingRowIndex <- sample(1:nrow(diabetes), 0.8*nrow(diabetes))  # row indices for training data\n",
    "trainingData <- diabetes[trainingRowIndex, ]  # model training data\n",
    "testData  <- diabetes[-trainingRowIndex, ]   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmMod <- lm(Insulin ~ Glucose, data=trainingData)  # build the model\n",
    "distPred <- predict(lmMod, testData)\n",
    "summary (lmMod)\n",
    "91.69^2#8407.056\n",
    "AIC (lmMod)  ##smaller than original model - good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will evaluate accuracy by correlating the actuals and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_preds <- data.frame(cbind(actuals=testData$Insulin, \n",
    "                                  predicteds=distPred))  # make actuals_predicteds dataframe.\n",
    "correlation_accuracy <- cor(actuals_preds)  # 55.01%\n",
    "head(actuals_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(actuals_preds$actuals, actuals_preds$predicteds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also calculate the ration between the minum and maxumum predictions which another metric of accuracy as well as the mean absolute Percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))  \n",
    "# => 70.14%, min_max accuracy\n",
    "mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)  \n",
    "# => 61.59%, mean absolute percentage error, relatively high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do now a more strict analysis of the data by using the k-fold Cross validation technique, where we split the data in n folds and use each fold except one as our training set and evaluate each training set agains the testing fold. As our predictor we will obtain the mean of all MSEs from each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(DAAG)\n",
    "cvResults <- CVlm(data=diabetes, form.lm=Insulin ~ Glucose, \n",
    "                                   m=10, dots=T, seed=29, legend.pos=\"topleft\",  \n",
    "                                   printit=T, main=\"Small symbols are predicted values while bigger ones are actuals.\");# performs the CV\n",
    "attr(cvResults, 'ms')  # => 8529.453 mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to follow an example directed to understand the use of tunable parameters to estimate the accuracy of the model.\n",
    "\n",
    "First, we are going to simulate 100 training sets of size 50 from a polynomial regression model, and we will run a polynomial using a cubic spline models with degrees of freedom from 1 to 30 (the tunable parameter). \n",
    "\n",
    "### Remember, in this first exercise we are not splitting data from the dataset  to produced the training data, what we are doing is to get data obtained from the same polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and test samples\n",
    "seed <- 1809\n",
    "set.seed(seed)\n",
    " \n",
    "##Write function to generate x and y dataframe using polynomial x variable taken \n",
    "#from a uniform distribution.\n",
    "gen_data <- function(n, beta, sigma_eps) {\n",
    "    eps <- rnorm(n, 0, sigma_eps)\n",
    "    x <- sort(runif(n, 0, 100))\n",
    "    X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))\n",
    "    y <- as.numeric(X %*% beta + eps)\n",
    "    \n",
    "    return(data.frame(x = x, y = y))\n",
    "}\n",
    " \n",
    "# Fit the models\n",
    "require(splines)\n",
    " \n",
    "n_rep <- 100\n",
    "n_df <- 30\n",
    "df <- 1:n_df\n",
    "beta <- c(5, -0.1, 0.004, -3e-05) ##These are the different values of beta that will help fit the model to a polynomial\n",
    "n_train <- 50\n",
    "n_test <- 10000\n",
    "sigma_eps <- 0.5\n",
    " \n",
    "xy <- res <- list()\n",
    "xy_test <- gen_data(n_test, beta, sigma_eps)\n",
    "for (i in 1:n_rep) {\n",
    "    xy[[i]] <- gen_data(n_train, beta, sigma_eps)\n",
    "    x <- xy[[i]][, \"x\"]\n",
    "    y <- xy[[i]][, \"y\"]\n",
    "    res[[i]] <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf))) ##Generate a Basis Matrix for \n",
    "        #Natural Cubic Splines from 1 to 30 degrees of freedom\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xy[[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we have our test set and 100 training sets, and that we produced polynomial regressions using different values of splines for the model. Lets visualize how we did on the extreme splines (1,4, and 25) for one of the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "x <- xy[[1]]$x\n",
    "X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))\n",
    "y <- xy[[1]]$y\n",
    "plot(y ~ x, col = \"gray\", lwd = 2)\n",
    "lines(x, X %*% beta, lwd = 3, col = \"black\")\n",
    "lines(x, fitted(res[[1]][[1]]), lwd = 3, col = \"palegreen3\")\n",
    "lines(x, fitted(res[[1]][[4]]), lwd = 3, col = \"darkorange\")\n",
    "lines(x, fitted(res[[1]][[25]]), lwd = 3, col = \"steelblue\")\n",
    "legend(x = \"topleft\", legend = c(\"True function\", \"Linear fit (df = 1)\", \"Best model (df = 4)\", \n",
    "    \"Overfitted model (df = 25)\"), lwd = rep(3, 4), col = c(\"black\", \"palegreen3\", \n",
    "    \"darkorange\", \"steelblue\"), text.width = 32, cex = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[[1]][1]\n",
    "colfunc <- colorRampPalette(c(\"blue\", \"red\"))\n",
    "colors = colfunc(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(y ~ x, col = \"gray\", lwd = 2)\n",
    "lines(x, X %*% beta, lwd = 3, col = \"black\")\n",
    "for (i in 1:30){\n",
    "    lines(x, fitted(res[[1]][[i]]), lwd = 3, col = colors[i])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate for each training set and model the MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the training and test errors for each model\n",
    "pred <- list()\n",
    "mse <- te <- matrix(NA, nrow = n_df, ncol = n_rep)\n",
    "for (i in 1:n_rep) {\n",
    "    mse[, i] <- sapply(res[[i]], function(obj) deviance(obj)/nobs(obj))\n",
    "    pred[[i]] <- mapply(function(obj, degf) predict(obj, data.frame(x = xy_test$x)), \n",
    "        res[[i]], df)\n",
    "    te[, i] <- sapply(as.list(data.frame(pred[[i]])), function(y_hat) mean((xy_test$y - \n",
    "        y_hat)^2))\n",
    "}\n",
    " \n",
    "# Compute the average training and test errors\n",
    "av_mse <- rowMeans(mse)\n",
    "av_te <- rowMeans(te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "plot(df, av_mse, type = \"l\", lwd = 2, col = gray(0.4), ylab = \"Prediction error\", \n",
    "    xlab = \"Flexibilty (spline's degrees of freedom [log scaled])\", ylim = c(0, \n",
    "        1), log = \"x\")\n",
    "abline(h = sigma_eps, lty = 2, lwd = 0.5)\n",
    "for (i in 1:n_rep) {\n",
    "    lines(df, te[, i], col = \"lightpink\")\n",
    "}\n",
    "for (i in 1:n_rep) {\n",
    "    lines(df, mse[, i], col = gray(0.8))\n",
    "}\n",
    "lines(df, av_mse, lwd = 2, col = gray(0.4))\n",
    "lines(df, av_te, lwd = 2, col = \"darkred\")\n",
    "points(df[1], av_mse[1], col = \"palegreen3\", pch = 17, cex = 1.5)\n",
    "points(df[1], av_te[1], col = \"palegreen3\", pch = 17, cex = 1.5)\n",
    "points(df[which.min(av_te)], av_mse[which.min(av_te)], col = \"darkorange\", pch = 16, \n",
    "    cex = 1.5)\n",
    "points(df[which.min(av_te)], av_te[which.min(av_te)], col = \"darkorange\", pch = 16, \n",
    "    cex = 1.5)\n",
    "points(df[25], av_mse[25], col = \"steelblue\", pch = 15, cex = 1.5)\n",
    "points(df[25], av_te[25], col = \"steelblue\", pch = 15, cex = 1.5)\n",
    "legend(x = \"top\", legend = c(\"Training error\", \"Test error\"), lwd = rep(2, 2), \n",
    "    col = c(gray(0.4), \"darkred\"), text.width = 0.3, cex = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that:\n",
    "\n",
    "1. the training errors decrease monotonically as the model gets more complicated (and less smooth).\n",
    "\n",
    "2. The test error initially decreases, and starts increasing again. The change point occurs in correspondence of the orange model, that is, the model that provides a good compromise between bias and variance. \n",
    "\n",
    "3. The reason why the test error starts increasing for degrees of freedom larger than 3 or 4 is the so called overfitting problem. Overfitting is the tendency of a model to adapt too well to the training data, at the expense of generalization to previously unseen data points. In other words, an overfitted model fits the noise in the data rather than the actual underlying relationships among the variables. Overfitting usually occurs when a model is unnecessarily complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to effectively evaluate the model accuracy we then use Cross-Validation\n",
    "\n",
    "# K-fold Cross validation\n",
    "\n",
    "In this approach we randomly split the observations into k groups, or approximately the same size. The first fold or group is treated as the validation set, and the method is fit on the remaining k-1 methods. On each fold we compute the respective MSE and the final MSE value is averaged and compared with the average of the test set from .model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(seed)\n",
    " \n",
    "n_train <- 100\n",
    "xy <- gen_data(n_train, beta, sigma_eps)\n",
    "x <- xy$x\n",
    "y <- xy$y\n",
    " \n",
    "fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))\n",
    "mse <- sapply(fitted_models, function(obj) deviance(obj)/nobs(obj))\n",
    " \n",
    "n_test <- 10000\n",
    "xy_test <- gen_data(n_test, beta, sigma_eps)\n",
    "pred <- mapply(function(obj, degf) predict(obj, data.frame(x = xy_test$x)), \n",
    "    fitted_models, df)\n",
    "te <- sapply(as.list(data.frame(pred)), function(y_hat) mean((xy_test$y - y_hat)^2))\n",
    "\n",
    "    \n",
    "n_folds <- 10\n",
    "folds_i <- sample(rep(1:n_folds, length.out = n_train))\n",
    "cv_tmp <- matrix(NA, nrow = n_folds, ncol = length(df))\n",
    "for (k in 1:n_folds) {\n",
    "    test_i <- which(folds_i == k)\n",
    "    train_xy <- xy[-test_i, ]\n",
    "    test_xy <- xy[test_i, ]\n",
    "    x <- train_xy$x\n",
    "    y <- train_xy$y\n",
    "    fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))\n",
    "    x <- test_xy$x\n",
    "    y <- test_xy$y\n",
    "    pred <- mapply(function(obj, degf) predict(obj, data.frame(ns(x, df = degf))), \n",
    "        fitted_models, df)\n",
    "    cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y - \n",
    "        y_hat)^2))\n",
    "}\n",
    "cv <- colMeans(cv_tmp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(Hmisc)\n",
    " \n",
    "plot(df, mse, type = \"l\", lwd = 2, col = gray(0.4), ylab = \"Prediction error\", \n",
    "    xlab = \"Flexibilty (spline's degrees of freedom [log scaled])\", main = paste0(n_folds, \n",
    "        \"-fold Cross-Validation\"), ylim = c(0.1, 0.8), log = \"x\")\n",
    "lines(df, te, lwd = 2, col = \"darkred\", lty = 2)\n",
    "cv_sd <- apply(cv_tmp, 2, sd)/sqrt(n_folds)\n",
    "errbar(df, cv, cv + cv_sd, cv - cv_sd, add = TRUE, col = \"steelblue2\", pch = 19, \n",
    "    lwd = 0.5)\n",
    "lines(df, cv, lwd = 2, col = \"steelblue2\")\n",
    "points(df, cv, col = \"steelblue2\", pch = 19)\n",
    "legend(x = \"topright\", legend = c(\"Training error\", \"Test error\", \"Cross-validation error\"), \n",
    "    lty = c(1, 2, 1), lwd = rep(2, 3), col = c(gray(0.4), \"darkred\", \"steelblue2\"), \n",
    "    text.width = 0.4, cex = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work through a more concrete example continue the tutorial from [http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/](http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/). to the section called:\n",
    "\n",
    "Doing Cross-Validation With R: the caret Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This website has a good tutorial for logistic regression that implements the cost function and gradient descend.\n",
    "[http://pingax.com/logistic-regression-r-step-step-implementation-part-2/](http://pingax.com/logistic-regression-r-step-step-implementation-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
