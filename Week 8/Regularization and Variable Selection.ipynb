{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Variable Selection in linear models (Shrinkage Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO, Ridge and Elastic Net\n",
    "\n",
    "\n",
    "### Ridge\n",
    "Ridge uses a tunable parameter that is used to minimized the coefficients as low as possible using this equation.\n",
    "\n",
    "$$RSS(residual sum of squares) + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "$\\lambda  is \\leq 0$ In this case ridge coefficients are adjusted to fit the data well but there is no really reduction of coefficients unless there contribution to the model is almost 0. Similar to the least squares technique for linear model ridge tries to minimize the RSS but it adds and adittional paramenter called the shrinkage penalty and this tends to reduce the coefficients toward zero.\n",
    "\n",
    "###LASSO\n",
    "LASSO relies open the linear model to reduce features using an alternative fitting procedure for estimating the coefficients. This procedure is more restrictive and reduce the coefficients that do not contribute to the model to 0.\n",
    "\n",
    "It also uses a tunable parameter $\\lambda$\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "LASSO uses an L1 penalty instead of an L2. as mentioned before the shrinkage penalty in lasso tends to recude some variables to zero, if they have very low coefficients to beging with.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "As we will explore next, LASSO present problems whit highly correlated datasets or high dimensional data where LASSO tends to select one variable from a group and ignore the others. Elastic net overcomes this difficulties by addind a quadratic term to the shrinkage penalty \n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the tutorial from [https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html](https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html). That gives a very good explanation to the differences and strenghts to the three different methods for variable selection.\n",
    "\n",
    "In the first section we will simulate data taken from a normal distribution, where there is very small signal and lots of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)  # Package needed to generate correlated precictors\n",
    "library(glmnet)  # Package to fit ridge/lasso/elastic net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "set.seed(19875)  # Set seed for reproducibility\n",
    "n <- 1000  # Number of observations\n",
    "p <- 5000  # Number of predictors included in model\n",
    "real_p <- 15  # Number of true predictors\n",
    "x <- matrix(rnorm(n*p), nrow=n, ncol=p)\n",
    "y <- apply(x[,1:real_p], 1, sum) + rnorm(n)\n",
    "\n",
    "# Split data into train (2/3) and test (1/3) sets\n",
    "train_rows <- sample(1:n, .66*n)\n",
    "x.train <- x[train_rows, ]\n",
    "x.test <- x[-train_rows, ]\n",
    "\n",
    "y.train <- y[train_rows]\n",
    "y.test <- y[-train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models \n",
    "# (For plots on left):\n",
    "fit.lasso <- glmnet(x.train, y.train, family=\"gaussian\", alpha=1)\n",
    "fit.ridge <- glmnet(x.train, y.train, family=\"gaussian\", alpha=0)\n",
    "fit.elnet <- glmnet(x.train, y.train, family=\"gaussian\", alpha=.5)\n",
    "\n",
    "\n",
    "# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0\n",
    "# (For plots on Right)\n",
    "for (i in 0:10) {\n",
    "    assign(paste(\"fit\", i, sep=\"\"), cv.glmnet(x.train, y.train, type.measure=\"mse\", \n",
    "                                              alpha=i/10,family=\"gaussian\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution paths:\n",
    "par(mfrow=c(3,2))\n",
    "# For plotting options, type '?plot.glmnet' in R console\n",
    "plot(fit.lasso, xvar=\"lambda\")\n",
    "plot(fit10, main=\"LASSO\")\n",
    "\n",
    "plot(fit.ridge, xvar=\"lambda\")\n",
    "plot(fit0, main=\"Ridge\")\n",
    "\n",
    "plot(fit.elnet, xvar=\"lambda\")\n",
    "plot(fit5, main=\"Elastic Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE on test set\n",
    "yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=x.test)\n",
    "yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=x.test)\n",
    "yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=x.test)\n",
    "yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=x.test)\n",
    "yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=x.test)\n",
    "yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=x.test)\n",
    "yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=x.test)\n",
    "yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=x.test)\n",
    "yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=x.test)\n",
    "yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=x.test)\n",
    "yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=x.test)\n",
    "\n",
    "mse0 <- mean((y.test - yhat0)^2)\n",
    "mse1 <- mean((y.test - yhat1)^2)\n",
    "mse2 <- mean((y.test - yhat2)^2)\n",
    "mse3 <- mean((y.test - yhat3)^2)\n",
    "mse4 <- mean((y.test - yhat4)^2)\n",
    "mse5 <- mean((y.test - yhat5)^2)\n",
    "mse6 <- mean((y.test - yhat6)^2)\n",
    "mse7 <- mean((y.test - yhat7)^2)\n",
    "mse8 <- mean((y.test - yhat8)^2)\n",
    "mse9 <- mean((y.test - yhat9)^2)\n",
    "mse10 <- mean((y.test - yhat10)^2)\n",
    "\n",
    "mse0 ##Ridge\n",
    "mse2\n",
    "mse4\n",
    "mse8\n",
    "mse10 ##Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO is the winner! LASSO is good at picking up a small signal through lots of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part we will simulate new data, where the signal is strong but there is also lots of noice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)  # Package needed to generate correlated precictors\n",
    "library(glmnet)  # Package to fit ridge/lasso/elastic net models\n",
    "\n",
    "# Generate data\n",
    "set.seed(19874)\n",
    "n <- 1000    # Number of observations\n",
    "p <- 5000     # Number of predictors included in model\n",
    "real_p <- 1500  # Number of true predictors\n",
    "x <- matrix(rnorm(n*p), nrow=n, ncol=p)\n",
    "y <- apply(x[,1:real_p], 1, sum) + rnorm(n)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_rows <- sample(1:n, .66*n)\n",
    "x.train <- x[train_rows, ]\n",
    "x.test <- x[-train_rows, ]\n",
    "\n",
    "y.train <- y[train_rows]\n",
    "y.test <- y[-train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT the Model\n",
    "\n",
    "# Fit models:\n",
    "fit.lasso <- glmnet(x.train, y.train, family=\"gaussian\", alpha=1)\n",
    "fit.ridge <- glmnet(x.train, y.train, family=\"gaussian\", alpha=0)\n",
    "fit.elnet <- glmnet(x.train, y.train, family=\"gaussian\", alpha=.5)\n",
    "\n",
    "\n",
    "# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0\n",
    "fit.lasso.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=1, \n",
    "                          family=\"gaussian\")\n",
    "fit.ridge.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=0,\n",
    "                          family=\"gaussian\")\n",
    "fit.elnet.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=.5,\n",
    "                          family=\"gaussian\")\n",
    "\n",
    "for (i in 0:10) {\n",
    "    assign(paste(\"fit\", i, sep=\"\"), cv.glmnet(x.train, y.train, type.measure=\"mse\", \n",
    "                                              alpha=i/10,family=\"gaussian\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution paths:\n",
    "par(mfrow=c(3,2))\n",
    "# For plotting options, type '?plot.glmnet' in R console\n",
    "plot(fit.lasso, xvar=\"lambda\")\n",
    "plot(fit10, main=\"LASSO\")\n",
    "\n",
    "plot(fit.ridge, xvar=\"lambda\")\n",
    "plot(fit0, main=\"Ridge\")\n",
    "\n",
    "plot(fit.elnet, xvar=\"lambda\")\n",
    "plot(fit5, main=\"Elastic Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=x.test)\n",
    "yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=x.test)\n",
    "yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=x.test)\n",
    "yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=x.test)\n",
    "yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=x.test)\n",
    "yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=x.test)\n",
    "yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=x.test)\n",
    "yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=x.test)\n",
    "yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=x.test)\n",
    "yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=x.test)\n",
    "yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=x.test)\n",
    "\n",
    "mse0 <- mean((y.test - yhat0)^2)\n",
    "mse1 <- mean((y.test - yhat1)^2)\n",
    "mse2 <- mean((y.test - yhat2)^2)\n",
    "mse3 <- mean((y.test - yhat3)^2)\n",
    "mse4 <- mean((y.test - yhat4)^2)\n",
    "mse5 <- mean((y.test - yhat5)^2)\n",
    "mse6 <- mean((y.test - yhat6)^2)\n",
    "mse7 <- mean((y.test - yhat7)^2)\n",
    "mse8 <- mean((y.test - yhat8)^2)\n",
    "mse9 <- mean((y.test - yhat9)^2)\n",
    "mse10 <- mean((y.test - yhat10)^2)\n",
    "\n",
    "mse0 ##Ridge\n",
    "mse2\n",
    "mse4\n",
    "mse8\n",
    "mse10 ##Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge is the winner! Ridge in general is good at prediction, but is not very interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will evaluate when the signal is variable but there is high correlation among the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "set.seed(19873)\n",
    "n <- 100    # Number of observations\n",
    "p <- 50     # Number of predictors included in model\n",
    "CovMatrix <- outer(1:p, 1:p, function(x,y) {.7^abs(x-y)})\n",
    "x <- mvrnorm(n, rep(0,p), CovMatrix)\n",
    "y <- 10 * apply(x[, 1:2], 1, sum) + \n",
    "  5 * apply(x[, 3:4], 1, sum) +\n",
    "  apply(x[, 5:14], 1, sum) +\n",
    "  rnorm(n)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_rows <- sample(1:n, .66*n)\n",
    "x.train <- x[train_rows, ]\n",
    "x.test <- x[-train_rows, ]\n",
    "\n",
    "y.train <- y[train_rows]\n",
    "y.test <- y[-train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models:\n",
    "fit.lasso <- glmnet(x.train, y.train, family=\"gaussian\", alpha=1)\n",
    "fit.ridge <- glmnet(x.train, y.train, family=\"gaussian\", alpha=0)\n",
    "fit.elnet <- glmnet(x.train, y.train, family=\"gaussian\", alpha=.5)\n",
    "\n",
    "\n",
    "# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0\n",
    "fit.lasso.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=1, \n",
    "                          family=\"gaussian\")\n",
    "fit.ridge.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=0,\n",
    "                          family=\"gaussian\")\n",
    "fit.elnet.cv <- cv.glmnet(x.train, y.train, type.measure=\"mse\", alpha=.5,\n",
    "                          family=\"gaussian\")\n",
    "\n",
    "for (i in 0:10) {\n",
    "    assign(paste(\"fit\", i, sep=\"\"), cv.glmnet(x.train, y.train, type.measure=\"mse\", \n",
    "                                              alpha=i/10,family=\"gaussian\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution paths:\n",
    "par(mfrow=c(3,2))\n",
    "# For plotting options, type '?plot.glmnet' in R console\n",
    "plot(fit.lasso, xvar=\"lambda\")\n",
    "plot(fit10, main=\"LASSO\")\n",
    "\n",
    "plot(fit.ridge, xvar=\"lambda\")\n",
    "plot(fit0, main=\"Ridge\")\n",
    "\n",
    "plot(fit.elnet, xvar=\"lambda\")\n",
    "plot(fit5, main=\"Elastic Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat0 <- predict(fit0, s=fit0$lambda.1se, newx=x.test)\n",
    "yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=x.test)\n",
    "yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=x.test)\n",
    "yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=x.test)\n",
    "yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=x.test)\n",
    "yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=x.test)\n",
    "yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=x.test)\n",
    "yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=x.test)\n",
    "yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=x.test)\n",
    "yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=x.test)\n",
    "yhat10 <- predict(fit10, s=fit10$lambda.1se, newx=x.test)\n",
    "\n",
    "mse0 <- mean((y.test - yhat0)^2)\n",
    "mse1 <- mean((y.test - yhat1)^2)\n",
    "mse2 <- mean((y.test - yhat2)^2)\n",
    "mse3 <- mean((y.test - yhat3)^2)\n",
    "mse4 <- mean((y.test - yhat4)^2)\n",
    "mse5 <- mean((y.test - yhat5)^2)\n",
    "mse6 <- mean((y.test - yhat6)^2)\n",
    "mse7 <- mean((y.test - yhat7)^2)\n",
    "mse8 <- mean((y.test - yhat8)^2)\n",
    "mse9 <- mean((y.test - yhat9)^2)\n",
    "mse10 <- mean((y.test - yhat10)^2)\n",
    "\n",
    "mse0 ##Ridge\n",
    "mse2\n",
    "mse4\n",
    "mse8\n",
    "mse10 ##Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case Ridge was the worse shrinkage method, eventhough the diagnostic plots between lasso and elastic nets are very similar. Elastic Net is the winner! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folowing tutorial, the authors demonstrate the use of lasso in the logistic regresion. \n",
    "\n",
    "In this cases we will be transforming the continuous outcome to discrete, and the evaluation to the model will rely on the constrast differences between the categorical predictors. \n",
    "\n",
    "https://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
