{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Case\n",
    "\n",
    "In order to understand a little better how to utilize unsupervised machine learning algorithms we will runa test case that comes from the final project presented by out TA Soyoung An and Yisurai Du for BMI 6018 Intro to Programming\n",
    "\n",
    "The project title is: \n",
    "### Relationship between amount of 911 calls and characteristics of townships in Montgomery County in Pennsylvania\n",
    "\n",
    "The main goal of the project is to try to understand the factors associated to 911 calls in the Montgomery Count in Pennsylvania.\n",
    "\n",
    "There are 4 predictors included:\n",
    "\n",
    "Levels of education (high - low)\n",
    "employment (yes - no)\n",
    "race (White - others)\n",
    "income (quantitative)\n",
    "\n",
    "The data that we will use is a set already collected and prepared. The data is also normalized by the population size for each township (total of 67 townships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)  # data manipulation\n",
    "library(cluster)    # clustering algorithms\n",
    "#install.packages(\"factoextra\")\n",
    "library(factoextra)\n",
    "library(dendextend)\n",
    "library(psych)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this twice to get red of the first character column\n",
    "calls = read.csv(file = \"911calls.csv\")\n",
    "row.names(calls) = calls$twp\n",
    "calls = calls[,-1]\n",
    "calls = calls[,-1]\n",
    "head(calls,20)\n",
    "calls[rownames(calls)==\"NARBERTH\",]\n",
    "calls[rownames(calls)==\"PHILADELPHIA\",]\n",
    "#biplot(pca_result, scale = 0, cex = 0.5) ## After you run the PCA you can use this to compare the values in \n",
    "#some of the counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's see how correlated are the variables among each other and specially with 911 calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##These are two functions that give pair-wise correlation coefficients for every variable\n",
    "round(cor(calls),4)\n",
    "pairs.panels(calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that rate_call_911 is not strongly associated to any variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scale(calls)\n",
    "distance_calls <- get_dist(scaled_df) #get_dist(): Computes a distance matrix between the rows of a data matrix.\n",
    "fviz_dist(distance_calls, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There seems to be strong clustering around some of the counties, maybe we shoud subset based on some parameter, lets do a PCA and see the distribution across the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by examining the data using a PCA to see if we can use PCs in order to minimize the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####PCA###\n",
    "apply(calls, 2, var)\n",
    "scaled_df <- apply(calls, 2, scale)\n",
    "head(scaled_df)\n",
    "row.names(scaled_df) = row.names(calls)\n",
    "glimpse(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cor(scaled_df),4)\n",
    "pairs.panels(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##There is a clear outlier that is driving the distribution in the rate of employment variable\n",
    "##Should we eliminate this outlier??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.cov <- cov(scaled_df)\n",
    "arrests.eigen <- eigen(arrests.cov)\n",
    "str(arrests.eigen)\n",
    "\n",
    "(phi <- arrests.eigen$vectors[,1:4])\n",
    "\n",
    "phi <- -phi\n",
    "row.names(phi) <- c(\"rate_high_edu\", \"rate_call_911\", \"rate_emp\", \"rate_Race_w\", \"income\")\n",
    "colnames(phi) <- c(\"PC1\", \"PC2\",\"PC3\",\"PC4\")\n",
    "phi\n",
    "\n",
    "PC1 <- as.matrix(scaled_df) %*% phi[,1]\n",
    "PC2 <- as.matrix(scaled_df) %*% phi[,2]\n",
    "PC3 <- as.matrix(scaled_df) %*% phi[,3]\n",
    "PC4 <- as.matrix(scaled_df) %*% phi[,4]\n",
    "\n",
    "# Create data frame with Principal Components scores\n",
    "PC <- data.frame(State = row.names(scaled_df), PC1, PC2, PC3, PC4)\n",
    "head(PC)\n",
    "\n",
    "ggplot(PC, aes(PC1, PC2)) + \n",
    "  modelr::geom_ref_line(h = 0) +\n",
    "  modelr::geom_ref_line(v = 0) +\n",
    "  geom_point() +\n",
    "  #geom_text(aes(label = State), size = 3) +\n",
    "  xlab(\"First Principal Component\") + \n",
    "  ylab(\"Second Principal Component\") + \n",
    "  ggtitle(\"First Two Principal Components of USArrests Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you look at the loadings, we see that PC1 is dominated by rate_high_edu and income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVE <- arrests.eigen$values / sum(arrests.eigen$values)\n",
    "round(PVE, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC1 explains 41% of the variation and PC2 25% we only need these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result <- prcomp(scaled_df, scale = TRUE)\n",
    "names(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result$x <- - pca_result$x\n",
    "head(pca_result$x)\n",
    "\n",
    "biplot(pca_result, scale = 0, cex = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use the k-means technique to split the counties based on similarities across variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###K-Means###\n",
    "\n",
    "k3 <- kmeans(scaled_df, centers = 4, nstart = 25)\n",
    "head(k3$cluster)\n",
    "PC$Kmeans = k3$cluster\n",
    "PC$Kmeans = as.factor(PC$Kmeans)\n",
    "clusters_one = PC[PC$Kmeans == 1,] \n",
    "clusters_two = PC[PC$Kmeans == 2,] \n",
    "clusters_three = PC[PC$Kmeans == 3,] \n",
    "clusters_four = PC[PC$Kmeans == 4,] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I split the data in 4 groups based on the overall variance of the dataset. The plot below is how the counties separate. Remember that PCA1 is highly driven by income and rate_high_education PC2 Is dominated by race and employment, so group 1 counties have lower income and more race diversity and group 4 have higher income and medium diversity, group 2 is the less diverse of medium income to low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(PC, aes(PC1, PC2,color = Kmeans)) + \n",
    "  modelr::geom_ref_line(h = 0) +\n",
    "  modelr::geom_ref_line(v = 0) +\n",
    "  geom_point() +\n",
    "  #geom_text(aes(label = State), size = 3) +\n",
    "  xlab(\"First Principal Component\") + \n",
    "  ylab(\"Second Principal Component\") + \n",
    "  ggtitle(\"First Two Principal Components of 911 call rates Montgomery County Pennsylvania\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because income is the highest loading in PCA with -0.6468989, let's split the data with this variable in three groups and see if our correlations improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_K = cbind(scaled_df,PC$Kmeans)\n",
    "scaled_df_K = as.data.frame(scaled_df_K)\n",
    "\n",
    "\n",
    "low = scaled_df_K[scaled_df_K$V6==1,]\n",
    "low = low[,1:5]\n",
    "round(cor(low),4)\n",
    "pairs.panels(low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wow look at those correlations, there is a lot more power in this subset as income and rate_high_edu are highly correlated to rate_911_calls. Another important piece of information is the sign of the correlation. Income is negatively correlated which means that the higher the income the less number of 911 calls, similar to rate_high_edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = scaled_df_K[scaled_df_K$V6==3,]\n",
    "high = high[,1:5]\n",
    "round(cor(high),4)\n",
    "pairs.panels(high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# things look very different to our higher income counties, there is no really correlations across variables and the rates of 9_11 calls as expected as there is not a strong separationg of race and income in this group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = scaled_df_K[scaled_df_K$V6==4,]\n",
    "mid = mid[,1:5]\n",
    "round(cor(mid),4)\n",
    "pairs.panels(mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There even less correlations here, there might be some other components that could be driving this group that we havent been able to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's compare the k-means approach to the hierarchichal clustering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hierarchical Clustering\n",
    "\n",
    "# Dissimilarity matrix\n",
    "d <- dist(scaled_df, method = \"euclidean\")\n",
    "\n",
    "# Hierarchical clustering using Complete Linkage\n",
    "hc1 <- hclust(d, method = \"complete\" )\n",
    "\n",
    "# Plot the obtained dendrogram\n",
    "plot(hc1, cex = 0.6, hang = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are using an euclidean distance and a complete linkage clustering algorith, we can see 3 or 4 clear clusters with some strong outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to assess which is the best clustering method for this dataset\n",
    "m <- c( \"average\", \"single\", \"complete\", \"ward\")\n",
    "names(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n",
    "\n",
    "# function to compute coefficient\n",
    "ac <- function(x) {\n",
    "  agnes(scaled_df, method = x)$ac\n",
    "}\n",
    "\n",
    "map_dbl(m, ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The coefficients are very similar but it seems that the ward method might be better for this data. let's look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc3 <- agnes(scaled_df, method = \"ward\")\n",
    "pltree(hc3, cex = 0.6, hang = -1, main = \"Dendrogram of agnes\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltree(hc3, cex = 0.6, hang = -1, main = \"Dendrogram of agnes\")\n",
    "rect.hclust(hc3, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three clear large clusters and two outliers that might be introducing noise to the data. You can cut the data with these groups and get the correlations plots and see if the signal improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plot below is a different representation of the clusters from k-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(list(data = scaled_df_K, cluster = scaled_df_K$V6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_nbclust(scaled_df, FUN = hcut, method = \"wss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
